{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD-Leanirng ( SARSA and Q-Learning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # tqdm是显示循环进度条的库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。如下图所示，有一个 4×12 的网格世界，每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取 4 种动作：上、下、左、右。如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。\n",
    "\n",
    "![](cliff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))  # min表示不能超出网格右边界，max表示不能超出网格左边界\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,坐标轴原点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA:\n",
    "\n",
    "$Q(s,a)\\gets Q(s,a) +\\alpha[r+\\gamma Q(s',a')-Q(s,a)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    \"\"\" Sarsa算法 \"\"\"\n",
    "    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "\n",
    "    def take_action(self, state):  # 选取下一步的操作,具体实现为epsilon-贪婪\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "\n",
    "    def best_action(self, state):  # 用于打印策略\n",
    "        Q_max = np.max(self.Q_table[state])\n",
    "        a = [0 for _ in range(self.n_action)]  # a是一个长度为n_action的零向量\n",
    "        for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来\n",
    "            if self.Q_table[state, i] == Q_max:\n",
    "                a[i] = 1  # 找到对于当前state而言价值最大的action\n",
    "        return a\n",
    "\n",
    "    def update(self, s0, a0, r, s1, a1):\n",
    "        td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 12\n",
    "nrow = 4\n",
    "env = CliffWalkingEnv(ncol, nrow)\n",
    "np.random.seed(0)\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "agent = Sarsa(ncol, nrow, epsilon, alpha, gamma)\n",
    "num_episodes = 500  # 智能体在环境中运行的序列的数量\n",
    "\n",
    "return_list = []  # 记录每一条序列的回报\n",
    "for i in range(10):  # 显示10个进度条\n",
    "    # tqdm的进度条功能\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "            episode_return = 0\n",
    "            state = env.reset()\n",
    "            action = agent.take_action(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_action = agent.take_action(next_state)\n",
    "                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n",
    "                agent.update(state, action, reward, next_state, next_action)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            return_list.append(episode_return)\n",
    "            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)\n",
    "\n",
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Sarsa on {}'.format('Cliff Walking'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_agent(agent, env, action_meaning, disaster=[], end=[]):\n",
    "    for i in range(env.nrow):\n",
    "        for j in range(env.ncol):\n",
    "            if (i * env.ncol + j) in disaster:\n",
    "                print('****', end=' ')\n",
    "            elif (i * env.ncol + j) in end:\n",
    "                print('EEEE', end=' ')\n",
    "            else:\n",
    "                a = agent.best_action(i * env.ncol + j)  # i * env.ncol + j 代表当前的state\n",
    "                pi_str = ''\n",
    "                for k in range(len(action_meaning)):\n",
    "                    pi_str += action_meaning[k] if a[k] > 0 else 'o'\n",
    "                print(pi_str, end=' ')\n",
    "        print()\n",
    "\n",
    "\n",
    "action_meaning = ['^', 'v', '<', '>']\n",
    "print('Sarsa算法最终收敛得到的策略为：')\n",
    "print_agent(agent, env, action_meaning, list(range(37, 47)), [47])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Leaning:\n",
    "\n",
    "$Q(s,a) \\gets Q(s,a)+\\alpha[r+\\gamma \\max_{a'} Q(s',a') - Q(s,a)]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\" Q-learning算法 \"\"\"\n",
    "    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "\n",
    "    def take_action(self, state):  #选取下一步的操作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "\n",
    "    def best_action(self, state):  # 用于打印策略\n",
    "        Q_max = np.max(self.Q_table[state])\n",
    "        a = [0 for _ in range(self.n_action)]\n",
    "        for i in range(self.n_action):\n",
    "            if self.Q_table[state, i] == Q_max:\n",
    "                a[i] = 1\n",
    "        return a\n",
    "\n",
    "    def update(self, s0, a0, r, s1):  # 主要区别在于这里td error的计算方式\n",
    "        td_error = r + self.gamma * self.Q_table[s1].max() - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "agent = QLearning(ncol, nrow, epsilon, alpha, gamma)\n",
    "num_episodes = 500  # 智能体在环境中运行的序列的数量\n",
    "\n",
    "return_list = []  # 记录每一条序列的回报\n",
    "for i in range(10):  # 显示10个进度条\n",
    "    # tqdm的进度条功能\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "            episode_return = 0\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.take_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n",
    "                agent.update(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "            return_list.append(episode_return)\n",
    "            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)\n",
    "\n",
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Q-learning on {}'.format('Cliff Walking'))\n",
    "plt.show()\n",
    "\n",
    "action_meaning = ['^', 'v', '<', '>']\n",
    "print('Q-learning算法最终收敛得到的策略为：')\n",
    "print_agent(agent, env, action_meaning, list(range(37, 47)), [47])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80ce3dcf17b64634bcc4d988b6a72b43b2d7ca548d51474fce50865d16d0c6ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
